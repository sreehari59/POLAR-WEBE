{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Streamlit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjG0/1xjJxJnggaxa8neTo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreehari59/Polar-Embedding/blob/main/Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QVdtF4QOoxhW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "7c87c855-64a4-4b17-b243-bb649129a9b2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4a0c2836f609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from operator import index\n",
        "import streamlit as st\n",
        "from pandas.core.frame import DataFrame\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import DBSCAN\n",
        "from gsheetsdb import connect\n",
        "import plotly\n",
        "import colorlover as cl\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "st.title(\"Word Embeddings for Business Entities\")\n",
        "# option = st.sidebar.selectbox('Create your own polar pairs?',('Yes',  'No'))\n",
        "\n",
        "check = st.sidebar.selectbox('Check for', ('Bias', 'Hofstede'))\n",
        "\n",
        "\n",
        "def polar_list(list):\n",
        "  \n",
        "  right_polar_list = []\n",
        "  left_polar_list = []\n",
        "  for i in range(0,len(list)):\n",
        "    \n",
        "    left_polar_list.append(list[i][0].replace(\"-\",\"_\"))\n",
        "    right_polar_list.append(list[i][1].replace(\"-\",\"_\"))\n",
        "\n",
        "  return left_polar_list,right_polar_list\n",
        "\n",
        "\n",
        "def alphabetical_list_creation(list):\n",
        "  new_list = []\n",
        "  \n",
        "  for i in range(0,len(list)):\n",
        "    index_0 = list[i][0].replace(\"-\",\"_\")\n",
        "    index_1 = list[i][1].replace(\"-\",\"_\")\n",
        "    \n",
        "    if index_0 < index_1:\n",
        "      val = index_0+\"-\"+index_1\n",
        "      new_list.append(val)\n",
        "      \n",
        "    else:\n",
        "      val = index_1+\"-\"+index_0\n",
        "      new_list.append(val)\n",
        "      \n",
        "  return new_list\n",
        "\n",
        "def company_count(company_df,input_list,polar_embedding):  \n",
        "\n",
        "  # we then find the number of companies grouped on the basis of location\n",
        "  for i in input_list:       \n",
        "    j = i.replace(\"-\",\"\")    \n",
        "    j = j.replace(\"_\",\"-\")\n",
        "    \n",
        "    subset_df2 = polar_embedding[polar_embedding[j] < 0]\n",
        "    company_inclined_to_left_polar_df1 = subset_df2['Location'].value_counts()\n",
        "    left_polar = i.split(\"-\")[0]\n",
        "    \n",
        "    company_inclined_to_left_polar_df1 = pd.DataFrame({'Country':company_inclined_to_left_polar_df1.index, left_polar :company_inclined_to_left_polar_df1.values})\n",
        "    company_df=pd.merge(company_df, company_inclined_to_left_polar_df1, how='left',on='Country')    \n",
        "    company_df[left_polar] = round( company_df[left_polar] / company_df.iloc[:,1] * 100)\n",
        "\n",
        "    subset_df1 = polar_embedding[polar_embedding[j] > 0]\n",
        "    company_inclined_to_right_polar_df1 = subset_df1['Location'].value_counts()\n",
        "    right_polar = i.split(\"-\")[1]\n",
        "    \n",
        "    company_inclined_to_right_polar_df1 = pd.DataFrame({'Country':company_inclined_to_right_polar_df1.index, right_polar :company_inclined_to_right_polar_df1.values})\n",
        "    company_df=pd.merge(company_df, company_inclined_to_right_polar_df1, how='left',on='Country')    \n",
        "    company_df[right_polar] = round( company_df[right_polar] / company_df.iloc[:,1] * 100)\n",
        "\n",
        "\n",
        "  company_df = company_df.fillna(0)\n",
        "\n",
        "  # We are considering only the countries if the numberof companies in the country is over 3\n",
        "  company_df = company_df[company_df['Total Count'] > 3]\n",
        "\n",
        "  return company_df\n",
        "\n",
        "\n",
        "def polar_ranking(polar_list,total_score,ranking,company_df):\n",
        "  total_sum=0\n",
        "  total_sum_list=[]\n",
        "  polar_ranking_list = []\n",
        "  polar_index=0\n",
        "  for index,row in company_df.iterrows():  \n",
        "    \n",
        "    for i in polar_list:\n",
        "      \n",
        "      total_sum = total_sum + (row[i])\n",
        "    #print(company_df.iloc[index,2:])  \n",
        "    total_sum_list.append(total_sum/len(polar_list))\n",
        "    polar_ranking_list.append(index+1)\n",
        "    total_sum = 0\n",
        "\n",
        "  company_df[total_score] = total_sum_list\n",
        "  company_df= company_df.sort_values(by=[total_score],ascending=False)\n",
        "  company_df[ranking] = polar_ranking_list\n",
        "\n",
        "  return company_df\n",
        "\n",
        "def mean_absolute_error_score(merged_df,dimension):\n",
        "  MAE_of_Score = []\n",
        "  MAE_of_Score.append(mean_absolute_error(merged_df[dimension], merged_df[\"Total Score Random\"]))\n",
        "  MAE_of_Score.append(mean_absolute_error(merged_df[dimension], merged_df[\"Total Score Nearest Random\"]))\n",
        "  MAE_of_Score.append(mean_absolute_error(merged_df[dimension], merged_df[\"Total Score Human\"]))\n",
        "  MAE_of_Score.append(mean_absolute_error(merged_df[dimension], merged_df[\"Total Score Nearest Human\"]))\n",
        "  return MAE_of_Score\n",
        "\n",
        "def mean_absolute_error_rank(merged_df,dimension_ranking):\n",
        "  MAE = []\n",
        "  MAE.append(mean_absolute_error(merged_df[dimension_ranking], merged_df[\"Polar Rank R\"]))\n",
        "  MAE.append(mean_absolute_error(merged_df[dimension_ranking], merged_df[\"Polar Rank Nearest R\"]))\n",
        "  MAE.append(mean_absolute_error(merged_df[dimension_ranking], merged_df[\"Polar Rank H\"]))\n",
        "  MAE.append(mean_absolute_error(merged_df[dimension_ranking], merged_df[\"Polar Rank Nearest H\"]))\n",
        "  return MAE\n",
        "\n",
        "def correlation_calc(merged_df,dimension_ranking):\n",
        "  correlation = []\n",
        "  correlation.append(merged_df[\"Polar Rank R\"].corr(merged_df[dimension_ranking]))\n",
        "  correlation.append(merged_df[\"Polar Rank Nearest R\"].corr(merged_df[dimension_ranking]))\n",
        "  correlation.append(merged_df[\"Polar Rank H\"].corr(merged_df[dimension_ranking]))\n",
        "  correlation.append(merged_df[\"Polar Rank Nearest H\"].corr(merged_df[dimension_ranking]))\n",
        "  return correlation\n",
        "\n",
        "# Power Distance\n",
        "list_powerdistance_random =[('make', 'break'), ('cameraman', 'playwright'), ('mystical', 'factual'), ('promotional', 'defamation'), ('iconic', 'unknown')]\n",
        "nearest_random_list_powerdistance =[('making', 'breaking'), ('cameramen', 'dramatist'), ('magical', 'inaccuracies'), ('promo', 'libel'), ('recognizable', 'undetermined')]\n",
        "list_powerdistance =[('hierarchical','non-hierarchical'),('superior','equal'),('leader','subordinate'),('inequality','equality'),('autocrat','democrat')]\n",
        "nearest_human_list_powerdistance = [('hierarchy', 'consensus-based'), ('inferior', 'equalitys'), ('leaders', 'subordinates'), ('inequalities', 'equals'), ('autocratic', 'senator')]\n",
        "\n",
        "# Individualism\n",
        "list_individualism_random = [('lop', 'secure'), ('shah', 'poor'), ('pneumatic', 'solid'), ('interpret', 'misinterpret'), ('confer', 'refuse')]\n",
        "nearest_random_list_individualism= [('buri', 'securing'), ('ahmad', 'poorer'), ('hydraulic', 'consistent'), ('interpreting', 'misunderstand'), ('conferring', 'refusing')]\n",
        "list_individualism = [('individuality','community'),('self-interest','harmony'),('tasks','relationships'),('individual','groups'),('universalism','particularism')]\n",
        "nearest_human_list_individualism = [('originality', 'communities'), ('selfishness', 'harmonious'), ('task', 'relationship'), ('individuals', 'group'), ('mangxamba', 'unitarianism')]\n",
        "\n",
        "# Masculinity\n",
        "list_masculinity_random = [('try', 'abstain'), ('fatalistic', 'freewill'), ('knowledgeable', 'uninformed'), ('confine', 'free'), ('fan', 'warm')]\n",
        "nearest_random_list_masculinity = [('trying', 'abstaining'), ('nonchalant', 'gmv'), ('knowledgable', 'misinformed'), ('confining', 'freedom'), ('fans', 'cool')]\n",
        "list_masculinity = [('achievement', 'support'),('competitive', 'caring'),('assertive', 'submissive'),('ambitious', 'unambitious'),('sucess','cooperation')]\n",
        "nearest_human_list_masculinity = [('achievements', 'supported'), ('competition', 'loving'), ('forceful', 'subservient'), ('undertaking', 'unathletic'), ('ufauthor', 'bilateral')]\n",
        "\n",
        "# long term Orientation\n",
        "list_longterm_random = [('innovator', 'follower'), ('sensory', 'numb'), ('hedge', 'squander'), ('arachnid', 'serpent'), ('disclose', 'secrete')]\n",
        "nearest_random_list_longterm = [('visionary', 'disciple'), ('auditory', 'numbed'), ('fund', 'squandering'), ('itsy', 'serpents'), ('disclosing', 'secreted')]\n",
        "list_longterm = [('pragmatic','normative'),('progress','preserve'),('adapt','conserve'),('developing','stable'),('advance','retain')]\n",
        "nearest_human_list_longterm = [('pragmatism', 'conceptions'), ('efforts', 'preserving'), ('adapting', 'conserving'), ('develop', 'stability'), ('advancing', 'retained')]\n",
        "\n",
        "# Indulgence\n",
        "list_indulgence_random = [('diagnose', 'sicken'), ('intercourse', 'disconnection'), ('sensory', 'sensorial'), ('emasculate', 'strengthen'), ('metropolitan', 'rural')]\n",
        "nearest_random_list_indulgence = [('diagnosing', 'sickens'), ('sexual', 'disconnect'), ('auditory', 'skorokhod'), ('disempower', 'strengthening'), ('metro', 'urban')]\n",
        "list_indulgence = [('fulfillment','restriction'),('satisfaction','limitation'),('liberty','moderation'),('expand','direct'),('freedom','regulation')]\n",
        "nearest_human_list_indulgence = [('fulfilment', 'restrictions'), ('satisfied', 'limitations'), ('fredom', 'restraint'), ('expanding', 'indirect'), ('freedoms', 'regulations')]\n",
        "\n",
        "# Unceratinity Avoidance\n",
        "list_uncertainity_avoidance_random = [('stretcher', 'compressor'), ('amalgamate', 'separate'), ('caretaker', 'assailant'), ('taker', 'violator'), ('contaminate', 'sterilize')]\n",
        "nearest_random_list_uncertainity_avoidance = [('stretchers', 'compressors'), ('amalgamating', 'separately'), ('interim', 'assailants'), ('takers', 'violators'), ('contaminating', 'sterilized')]\n",
        "list_uncertainity_avoidance = [('clarity','complexity'),('clear','ambiguous'),('certain','uncertain'),('uniformity','diversity'),('agreement','variation')]\n",
        "nearest_human_list_uncertainity_avoidance = [('simplicity', 'complexities'), ('yet', 'vague'), ('particular', 'unclear'), ('homogeneity', 'diverse'), ('agreements', 'variations')]\n",
        "\n",
        "\n",
        "\n",
        "if (check == 'Hofstede'):\n",
        "    Hofstede_dimensions = st.sidebar.selectbox('Check for', ('Power Distance', 'Individualism vs Collectivism','Masculinity vs Femininity',\n",
        "                                                             'Long Term vs Short Term Orientation','Indulgence vs Restraint','Uncertainty Avoidance'))\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    new_df = pd.read_csv('/content/drive/MyDrive/Polar Embedding/120 dimension polar embedding.csv')\n",
        "    fortune_500_company = pd.read_csv('/content/drive/MyDrive/Fortune Global 500 companies.csv',encoding= 'unicode_escape')\n",
        "    fortune_500_company['Company'] = fortune_500_company['Company'].str.lower()\n",
        "    fortune_500_company['Company'] = fortune_500_company['Company'].str.replace(\" \", \"\")\n",
        "\n",
        "    polar_embedding = pd.merge(fortune_500_company, new_df, how=\"right\", left_on=\"Company\", right_on=\"Unnamed: 0\")\n",
        "\n",
        "    polar_embedding = polar_embedding.drop(['Rank'], axis=1)  # This will drop the column Rank\n",
        "    polar_embedding = polar_embedding.drop(['Unnamed: 0'], axis=1)  # This will drop the column Rank\n",
        "\n",
        "    # This will find the total number of companies in our data frame based on Location\n",
        "    total_company_list_based_on_loc = polar_embedding['Location'].value_counts()\n",
        "    total_company_count_df = pd.DataFrame({'Country': total_company_list_based_on_loc.index, 'Total Count': total_company_list_based_on_loc.values})\n",
        "\n",
        "    hofstede_df = pd.read_csv(\"/content/drive/MyDrive/Polar Embedding/Hofstede 6 dimensions.csv\",sep=\";\")\n",
        "    hofstede_df=hofstede_df[hofstede_df.iloc[:,:]!=\"#NULL!\" ]\n",
        "    \n",
        "        \n",
        "    dim_index = \"\"\n",
        "    dim_ranking = \"\"\n",
        "    if (Hofstede_dimensions == 'Power Distance'):\n",
        "      dim_index=\"Power distance index\"\n",
        "      dim_ranking=\"Power distance Ranking\"\n",
        "      \n",
        "      left_polar_list_random,right_polar_list_random = polar_list(list_powerdistance_random)\n",
        "      left_polar_list_nearest_random,right_polar_list_nearest_random = polar_list(nearest_random_list_powerdistance)\n",
        "      left_polar_list_human,right_polar_list_human = polar_list(list_powerdistance)\n",
        "      left_polar_list_nearest_human,right_polar_list_nearest_human = polar_list(nearest_human_list_powerdistance)\n",
        "\n",
        "\n",
        "      input_list_random = alphabetical_list_creation(list_powerdistance_random)\n",
        "      input_list_nearest_random = alphabetical_list_creation(nearest_random_list_powerdistance)\n",
        "      input_list_human = alphabetical_list_creation(list_powerdistance)\n",
        "      input_list_nearest_human = alphabetical_list_creation(nearest_human_list_powerdistance)\n",
        "\n",
        "    elif (Hofstede_dimensions == 'Individualism vs Collectivism'):\n",
        "      dim_index=\"Individualism index\"\n",
        "      dim_ranking=\"Individualism Ranking\"\n",
        "        \n",
        "\n",
        "      left_polar_list_random,right_polar_list_random = polar_list(list_individualism_random)\n",
        "      left_polar_list_nearest_random,right_polar_list_nearest_random = polar_list(nearest_random_list_individualism)\n",
        "      left_polar_list_human,right_polar_list_human = polar_list(list_individualism)\n",
        "      left_polar_list_nearest_human,right_polar_list_nearest_human = polar_list(nearest_human_list_individualism)\n",
        "\n",
        "\n",
        "      input_list_random = alphabetical_list_creation(list_individualism_random)\n",
        "      input_list_nearest_random = alphabetical_list_creation(nearest_random_list_individualism)\n",
        "      input_list_human = alphabetical_list_creation(list_individualism)\n",
        "      input_list_nearest_human = alphabetical_list_creation(nearest_human_list_individualism)\n",
        "\n",
        "    elif (Hofstede_dimensions == 'Masculinity vs Femininity'):\n",
        "      dim_index=\"Masculinity index\"\n",
        "      dim_ranking=\"Masculinity Ranking\"\n",
        "\n",
        "      \n",
        "      left_polar_list_random,right_polar_list_random = polar_list(list_masculinity_random)\n",
        "      left_polar_list_nearest_random,right_polar_list_nearest_random = polar_list(nearest_random_list_masculinity)\n",
        "      left_polar_list_human,right_polar_list_human = polar_list(list_masculinity)\n",
        "      left_polar_list_nearest_human,right_polar_list_nearest_human = polar_list(nearest_human_list_masculinity)\n",
        "\n",
        "\n",
        "      input_list_random = alphabetical_list_creation(list_masculinity_random)\n",
        "      input_list_nearest_random = alphabetical_list_creation(nearest_random_list_masculinity)\n",
        "      input_list_human = alphabetical_list_creation(list_masculinity)\n",
        "      input_list_nearest_human = alphabetical_list_creation(nearest_human_list_masculinity)\n",
        "        \n",
        "\n",
        "    elif (Hofstede_dimensions == 'Long Term vs Short Term Orientation'):\n",
        "      dim_index=\"Long term orientation index\"\n",
        "      dim_ranking=\"Long term orientation Ranking\"\n",
        "\n",
        "      \n",
        "      left_polar_list_random,right_polar_list_random = polar_list(list_longterm_random)\n",
        "      left_polar_list_nearest_random,right_polar_list_nearest_random = polar_list(nearest_random_list_longterm)\n",
        "      left_polar_list_human,right_polar_list_human = polar_list(list_longterm)\n",
        "      left_polar_list_nearest_human,right_polar_list_nearest_human = polar_list(nearest_human_list_longterm)\n",
        "\n",
        "\n",
        "      input_list_random = alphabetical_list_creation(list_longterm_random)\n",
        "      input_list_nearest_random = alphabetical_list_creation(nearest_random_list_longterm)\n",
        "      input_list_human = alphabetical_list_creation(list_longterm)\n",
        "      input_list_nearest_human = alphabetical_list_creation(nearest_human_list_longterm)\n",
        "        \n",
        "\n",
        "    elif (Hofstede_dimensions == 'Indulgence vs Restraint'):\n",
        "      dim_index=\"Indulgence index\"\n",
        "      dim_ranking=\"Indulgence Ranking\"\n",
        "\n",
        "      \n",
        "      left_polar_list_random,right_polar_list_random = polar_list(list_indulgence_random)\n",
        "      left_polar_list_nearest_random,right_polar_list_nearest_random = polar_list(nearest_random_list_indulgence)\n",
        "      left_polar_list_human,right_polar_list_human = polar_list(list_indulgence)\n",
        "      left_polar_list_nearest_human,right_polar_list_nearest_human = polar_list(nearest_human_list_indulgence)\n",
        "\n",
        "\n",
        "      input_list_random = alphabetical_list_creation(list_indulgence_random)\n",
        "      input_list_nearest_random = alphabetical_list_creation(nearest_random_list_indulgence)\n",
        "      input_list_human = alphabetical_list_creation(list_indulgence)\n",
        "      input_list_nearest_human = alphabetical_list_creation(nearest_human_list_indulgence)\n",
        "        \n",
        "\n",
        "    elif (Hofstede_dimensions == 'Uncertainty Avoidance'):\n",
        "      dim_index=\"Uncertainty avoidance index\"\n",
        "      dim_ranking=\"Uncertainty avoidance Ranking\"\n",
        "\n",
        "      \n",
        "      left_polar_list_random,right_polar_list_random = polar_list(list_uncertainity_avoidance_random)\n",
        "      left_polar_list_nearest_random,right_polar_list_nearest_random = polar_list(nearest_random_list_uncertainity_avoidance)\n",
        "      left_polar_list_human,right_polar_list_human = polar_list(list_uncertainity_avoidance)\n",
        "      left_polar_list_nearest_human,right_polar_list_nearest_human = polar_list(nearest_human_list_uncertainity_avoidance)\n",
        "\n",
        "\n",
        "      input_list_random = alphabetical_list_creation(list_uncertainity_avoidance_random)\n",
        "      input_list_nearest_random = alphabetical_list_creation(nearest_random_list_uncertainity_avoidance)\n",
        "      input_list_human = alphabetical_list_creation(list_uncertainity_avoidance)\n",
        "      input_list_nearest_human = alphabetical_list_creation(nearest_human_list_uncertainity_avoidance)\n",
        "        \n",
        "\n",
        "    company_df = total_company_count_df.copy()  # This make a copy of data frame\n",
        "    \n",
        "    #Below lines will find the number of companies aligned to the respective left word in antonym pair\n",
        "    company_df = company_count(company_df,input_list_random,polar_embedding)\n",
        "    company_df = company_count(company_df,input_list_nearest_random,polar_embedding)\n",
        "    company_df = company_count(company_df,input_list_human,polar_embedding)\n",
        "    company_df = company_count(company_df,input_list_nearest_human,polar_embedding)\n",
        "\n",
        "\n",
        "    #Below lines will find the total score based on the left word and final give a ranking\n",
        "    company_df = polar_ranking(left_polar_list_random,\"Total Score Random\",\"Polar Rank R\",company_df)\n",
        "    company_df = polar_ranking(left_polar_list_nearest_random,\"Total Score Nearest Random\",\"Polar Rank Nearest R\",company_df)\n",
        "    company_df = polar_ranking(left_polar_list_human,\"Total Score Human\",\"Polar Rank H\",company_df)\n",
        "    company_df = polar_ranking(left_polar_list_nearest_human,\"Total Score Nearest Human\",\"Polar Rank Nearest H\",company_df)\n",
        "\n",
        "    length = len(left_polar_list_random) + len(left_polar_list_nearest_random) + len(left_polar_list_human) + len(left_polar_list_nearest_human)\n",
        "    company_df.drop(company_df.iloc[:, 2:2 + (length) * 2], axis=1, inplace=True)\n",
        "\n",
        "    hofstede_df = hofstede_df[hofstede_df.iloc[:, :] != \"#NULL!\"]\n",
        "    hofstede_df.dropna(axis=0)\n",
        "\n",
        "    # This merge the company dataframe and Hofstede dataframe over the common column Country\n",
        "    merged_df = pd.merge(company_df, hofstede_df, how='left', on='Country')\n",
        "\n",
        "    ranking_list = []\n",
        "    for i in range(1, len(merged_df[dim_index]) + 1):\n",
        "        ranking_list.append(i)\n",
        "    merged_df = merged_df.sort_values(by=[dim_index], ascending=False)\n",
        "    merged_df[dim_ranking] = ranking_list\n",
        "\n",
        "    # Below are the correlation plot \n",
        "    sns.regplot(x=merged_df[dim_ranking], y=merged_df[\"Polar Rank R\"])\n",
        "    sns.regplot(x=merged_df[dim_ranking], y=merged_df[\"Polar Rank Nearest R\"])\n",
        "    sns.regplot(x=merged_df[dim_ranking], y=merged_df[\"Polar Rank H\"])\n",
        "    sns.regplot(x=merged_df[dim_ranking], y=merged_df[\"Polar Rank Nearest H\"])\n",
        "    \n",
        "\n",
        "\n",
        "    # Below is the Hofstede dimension score and our score we got for each of the 4 list\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig = make_subplots(rows=2, cols=2)\n",
        "\n",
        "    \n",
        "\n",
        "    fig.add_trace(go.Bar(x=merged_df[\"Country\"] , y=merged_df[dim_index].astype(int), name = dim_index),1,1)  \n",
        "    fig.add_trace(go.Bar(x=merged_df[\"Country\"] , y=merged_df[\"Total Score Random\"].astype(int), name = \"Random Polar Score\"),1,1)  \n",
        "\n",
        "    fig.add_trace(go.Bar(x=merged_df[\"Country\"] , y=merged_df[dim_index].astype(int), name = dim_index),1,2)  \n",
        "    fig.add_trace(go.Bar(x=merged_df[\"Country\"] , y=merged_df[\"Total Score Nearest Random\"].astype(int), name = \"Nearest Random Polar Score\"),1,2)  \n",
        "\n",
        "    fig.add_trace(go.Bar(x=merged_df[\"Country\"] , y=merged_df[dim_index].astype(int), name = dim_index),2,1)  \n",
        "    fig.add_trace(go.Bar(x=merged_df[\"Country\"] , y=merged_df[\"Total Score Human\"].astype(int), name = \"Human Polar Score\"),2,1)  \n",
        "\n",
        "    fig.add_trace(go.Bar(x=merged_df[\"Country\"] , y=merged_df[dim_index].astype(int), name = dim_index),2,2)  \n",
        "    fig.add_trace(go.Bar(x=merged_df[\"Country\"] , y=merged_df[\"Total Score Nearest Human\"].astype(int), name = \"Nearest Human Polar Score\"),2,2) \n",
        "\n",
        "    fig.show() \n",
        "\n",
        "    MAE = mean_absolute_error_rank(merged_df,dim_ranking)\n",
        "    MAE_of_Score = mean_absolute_error_rank(merged_df,dim_ranking)\n",
        "    correlation = correlation_calc(merged_df,dim_ranking)\n",
        "\n",
        "\n",
        "    # The below code creates a data frame with the results\n",
        "    eval_data = {\"Mean Absolute Error of Rank\" : MAE,\n",
        "                  \"Correlation\" : correlation,\n",
        "                \"Mean Absolute Error of Score\" : MAE_of_Score\n",
        "                }\n",
        "\n",
        "    eval_df = pd.DataFrame(eval_data, index =[\"Random List\", \"Nearest Random List\",\"Human Made List\",\"Nearest to Human Made List\"])\n",
        "    eval_df.head()\n",
        "\n",
        "    corr = merged_df.corr()\n",
        "    corr.style.background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1).highlight_null(null_color='#f1f1f1').set_precision(2)\n",
        "\n",
        "\n"
      ]
    }
  ]
}